{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"CS5785-final-data/captions.json\") as json_file:\n",
    "    captions_json = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yard' 'hill' 'ranch_house' ..., 'television_studio' 'street' 'bayou']\n"
     ]
    }
   ],
   "source": [
    "train_labels = np.genfromtxt(\"CS5785-final-data/train.txt\",dtype=None)\n",
    "train_labels = train_labels[:,1]\n",
    "labels_200 = np.array(list(set(train_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extract all nouns for each image\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "def extract_noun(sentence):\n",
    "    #lower case\n",
    "    lower = sentence.lower()\n",
    "    #split\n",
    "    text = nltk.tokenize.word_tokenize(lower)\n",
    "    #tag\n",
    "    tagged_text = nltk.pos_tag(text)\n",
    "    #extraxt noun\n",
    "    nouns = [w[0] for w in tagged_text if ((w[1]=='NN')|(w[1]=='NNS'))]\n",
    "    #singular form\n",
    "    nouns = [wnl.lemmatize(word,'n') for word in nouns]\n",
    "    return nouns\n",
    "\n",
    "jpg_nouns = []\n",
    "for jpg in captions_json.values():\n",
    "    result = [extract_noun(s) for s in jpg]\n",
    "    nouns = list(set([item for sublist in result for item in sublist]))\n",
    "    jpg_nouns.append(nouns)\n",
    "unique_nouns = list(set([item for sublist in jpg_nouns for item in sublist]))\n",
    "\n",
    "#extract nouns matching labels_200 for each image\n",
    "jpg_matched_words_10k = []\n",
    "for nouns in jpg_nouns:\n",
    "    #a is of length nouns\n",
    "    a =[(w in nouns) for w in labels_200]\n",
    "    matched_labels = []\n",
    "    if sum(a)>0:\n",
    "        matched_labels = labels_200[np.array(a)]\n",
    "    jpg_matched_words_10k.append(matched_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#phrases in labels_200\n",
    "phrases = [w for w in labels_200 if re.search(\"_\",w)]\n",
    "phrases = [re.sub(\"_\",\" \",w) for w in phrases]\n",
    "\n",
    "#images captions that include the phrases\n",
    "def match_phrase(sentence):\n",
    "    #lower case\n",
    "    lower = sentence.lower()\n",
    "    #regular expression to match phrase\n",
    "    matched_phrase = []\n",
    "    for ph in phrases:\n",
    "        if re.match(ph,lower):\n",
    "            matched_phrase.append(ph)\n",
    "    return matched_phrase\n",
    "\n",
    "jpg_matched_phrases_10k = []\n",
    "for jpg in captions_json.values():    \n",
    "    result = [match_phrase(s) for s in jpg]\n",
    "    phrase = list(set([item for sublist in result for item in sublist]))\n",
    "    jpg_matched_phrases_10k.append(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#indices of the images and corresponding words\n",
    "indices_words = [i for i, x in enumerate(jpg_matched_words_10k) if x != []]\n",
    "words = [jpg_matched_words_10k[i] for i in indices_words]\n",
    "#indices of of data and corresponding phrase\n",
    "indices_phrases = [i for i, x in enumerate(jpg_matched_phrases_10k) if x != []]\n",
    "phrases = [jpg_matched_phrases_10k[i] for i in indices_phrases]\n",
    "\n",
    "#change everything to np array\n",
    "indices_words = np.array(indices_words)\n",
    "indices_phrases = np.array(indices_phrases)\n",
    "wwords = np.array(words)\n",
    "phrases = np.array(phrases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print indices_words\n",
    "print words\n",
    "print indices_phrases\n",
    "print phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate the training data for each\n",
    "tenk_cnn = np.load(\"CS5785-final-data/alexnet_feat_10k.npy\")\n",
    "tenk_bow = np.load(\"CS5785-final-data/SIFTBoW_10k.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create features for tenk_cnn and tenk_bow with word labels\n",
    "#if a point have two labels, create two points with different labels\n",
    "\n",
    "tenk_cnn_words = tenk_cnn[indices_words]\n",
    "tenk_bow_words = tenk_bow[indices_words]\n",
    "\n",
    "rep = [len(k) for k in words]\n",
    "tenk_cnn_words_expand = np.repeat(tenk_cnn_words, rep, axis=0)\n",
    "tenk_bow_words_expand = np.repeat(tenk_bow_words, rep, axis=0)\n",
    "\n",
    "words_labels = [item for sublist in words for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sushi bar']\n",
      " ['construction site']\n",
      " ['baseball field']]\n"
     ]
    }
   ],
   "source": [
    "#add the three phrases\n",
    "print phrases\n",
    "phrases_labels = [\"sushi_bar\",\"construction_site\",\"baseball_field\"]\n",
    "\n",
    "tenk_cnn_phr = tenk_cnn[indices_phrases]\n",
    "tenk_bow_phr = tenk_bow[indices_phrases]\n",
    "\n",
    "tenk_feat_cnn_labelled = np.vstack((tenk_cnn_words_expand,tenk_cnn_phr))\n",
    "tenk_feat_bow_labelled = np.vstack((tenk_bow_words_expand,tenk_bow_phr))\n",
    "\n",
    "tenk_labels = words_labels + phrases_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5247L, 4096L)\n",
      "(5247L, 4096L)\n",
      "5247\n"
     ]
    }
   ],
   "source": [
    "print tenk_feat_cnn_labelled.shape\n",
    "print tenk_feat_bow_labelled.shape\n",
    "print len(tenk_labels)\n",
    "np.save(\"tenk_feat_cnn_labelled.npy\",tenk_feat_cnn_labelled)\n",
    "np.save(\"tenk_feat_bow_labelled.npy\",tenk_feat_bow_labelled)\n",
    "np.save(\"tenk_labels.npy\",tenk_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
