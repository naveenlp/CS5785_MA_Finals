{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run \"../library.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_train = preprocessing.normalize(np.hstack([train_cnn,train_bow]), axis=0)\n",
    "cluster_tenk = preprocessing.normalize(np.hstack([tenk_cnn,tenk_bow]), axis=0)\n",
    "full_data = np.vstack([cluster_train, cluster_tenk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# spectral clustering\n",
    "from sklearn import cluster\n",
    "\n",
    "spectral = cluster.SpectralClustering(n_clusters=200)\n",
    "spectral_output = spectral.fit(full_data)\n",
    "spectral_labels = spectral_output.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is a 13k by 13k affinity matrix. we can use this to get \n",
    "# affinity of training data with tenk data\n",
    "similarity_matrix = spectral.affinity_matrix_[:3000,3000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "port = PorterStemmer()\n",
    "lmtzr = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "def rstrip(l):\n",
    "    l = ''.join([i if ord(i) < 128 else ' ' for i in l])\n",
    "    return l.rstrip()\n",
    "\n",
    "\n",
    "def preprocess(l):\n",
    "    line = \" \".join(map(rstrip, l)).encode(\"ascii\")\n",
    "    words = tokenizer.tokenize(line)\n",
    "    \n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        # lowercase\n",
    "        new_word = word.lower()\n",
    "        # remove stop words\n",
    "        if new_word not in stopwords:\n",
    "            # stem and lemmatize\n",
    "            new_word = lmtzr.lemmatize(new_word)\n",
    "            new_word = port.stem(new_word)\n",
    "            # add to processed word list\n",
    "            processed_words.append(new_word)\n",
    "            \n",
    "    return processed_words\n",
    "\n",
    "words_data = map(preprocess, captions_json.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_dict_of_words(words_data):\n",
    "    word_set = Set()\n",
    "    for i in range(len(words_data)):\n",
    "        for word in words_data[i]:\n",
    "            word_set.add(word)\n",
    "            \n",
    "    return list(word_set)\n",
    "\n",
    "words_dict = get_dict_of_words(words_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7509"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tet = 0\n",
    "def get_word_count_data(words_data, words_dict):    \n",
    "    word_counts_data = []\n",
    "    for i in range(len(words_data)):\n",
    "        word_counts_row = np.zeros(len(words_dict))\n",
    "        \n",
    "        # store list of words for row\n",
    "        word_list = words_data[i]\n",
    "        \n",
    "        # store count of words for above list\n",
    "        word_counts = Counter(word_list)\n",
    "        \n",
    "        for w in word_counts.keys():\n",
    "            index = words_dict.index(w)\n",
    "            word_counts_row[index] = word_counts[w]\n",
    "            \n",
    "        word_counts_data.append(word_counts_row)\n",
    "            \n",
    "    return np.array(word_counts_data), tet\n",
    "\n",
    "# bag of words data\n",
    "word_counts_data, tet = get_word_count_data(words_data, words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize\n",
    "bow_data = normalize(word_counts_data, axis=1, norm='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 10000) (10000, 7509)\n"
     ]
    }
   ],
   "source": [
    "# take dot product\n",
    "print similarity_matrix.shape, bow_data.shape\n",
    "train_bow_data = similarity_matrix.dot(bow_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 7509)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can use this as our feature vector to do a logistic regression\n",
    "train_bow_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sets import Set\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import normalize\n",
    "import random\n",
    "from scipy.spatial import distance\n",
    "import scipy\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import heapq\n",
    "import string\n",
    "from sklearn.decomposition import PCA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
