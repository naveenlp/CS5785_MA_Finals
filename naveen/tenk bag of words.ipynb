{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run \"../library.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_train = np.hstack([train_cnn,train_bow])\n",
    "cluster_test = np.hstack([test_cnn,test_bow])\n",
    "cluster_tenk = np.hstack([tenk_cnn,tenk_bow])\n",
    "full_data = preprocessing.normalize(np.vstack([cluster_train, cluster_test, cluster_tenk]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# spectral clustering\n",
    "from sklearn import cluster\n",
    "\n",
    "spectral = cluster.SpectralClustering(n_clusters=200)\n",
    "spectral_output = spectral.fit(full_data)\n",
    "spectral_labels = spectral_output.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 10000) (1000, 10000)\n",
      "(3000, 10000) (1000, 10000)\n"
     ]
    }
   ],
   "source": [
    "# we have the 14k by 14k affinity matrix. we can use this to get \n",
    "# affinity of training data with tenk data and test data with tenk data\n",
    "similarity_matrix = spectral.affinity_matrix_[:3000,4000:]\n",
    "similarity_matrix_test = spectral.affinity_matrix_[3000:4000,4000:]\n",
    "print similarity_matrix.shape, similarity_matrix_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "port = PorterStemmer()\n",
    "lmtzr = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "def rstrip(l):\n",
    "    l = ''.join([i if ord(i) < 128 else ' ' for i in l])\n",
    "    return l.rstrip()\n",
    "\n",
    "\n",
    "def preprocess(l):\n",
    "    line = \" \".join(map(rstrip, l)).encode(\"ascii\")\n",
    "    words = tokenizer.tokenize(line)\n",
    "    \n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        # lowercase\n",
    "        new_word = word.lower()\n",
    "        # remove stop words\n",
    "        if new_word not in stopwords:\n",
    "            # stem and lemmatize\n",
    "            new_word = lmtzr.lemmatize(new_word)\n",
    "            new_word = port.stem(new_word)\n",
    "            # add to processed word list\n",
    "            processed_words.append(new_word)\n",
    "            \n",
    "    return processed_words\n",
    "\n",
    "words_data = map(preprocess, captions_json.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_dict_of_words(words_data):\n",
    "    word_set = Set()\n",
    "    for i in range(len(words_data)):\n",
    "        for word in words_data[i]:\n",
    "            word_set.add(word)\n",
    "            \n",
    "    return list(word_set)\n",
    "\n",
    "words_dict = get_dict_of_words(words_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of words/features: 7509\n",
      "total number of words/features: 7509\n"
     ]
    }
   ],
   "source": [
    "print \"total number of words/features:\", len(words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_word_count_data(words_data, words_dict, ignore = True):    \n",
    "    word_counts_data = []\n",
    "    for i in range(len(words_data)):\n",
    "        word_counts_row = np.zeros(len(words_dict))\n",
    "        \n",
    "        # store list of words for row\n",
    "        word_list = words_data[i]\n",
    "        \n",
    "        # prune to exclude words not in training data's bag of words\n",
    "        if not ignore:\n",
    "            word_list = list(set(word_list).intersection(words_dict))\n",
    "        \n",
    "        # store count of words for above list\n",
    "        word_counts = Counter(word_list)\n",
    "        \n",
    "        for w in word_counts.keys():\n",
    "            index = words_dict.index(w)\n",
    "            word_counts_row[index] = word_counts[w]\n",
    "            \n",
    "        word_counts_data.append(word_counts_row)\n",
    "            \n",
    "    return np.array(word_counts_data)\n",
    "\n",
    "# bag of words data\n",
    "word_counts_data = get_word_count_data(words_data, words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# normalize featureset\n",
    "bow_data = normalize(word_counts_data, axis=1, norm='l1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 10000) (10000, 7509)\n",
      "(3000, 10000) (10000, 7509)\n"
     ]
    }
   ],
   "source": [
    "# take dot product\n",
    "print similarity_matrix.shape, bow_data.shape\n",
    "train_bow_data = similarity_matrix.dot(bow_data)\n",
    "train_bow_data = preprocessing.scale(train_bow_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10000) (10000, 7509)\n",
      "(1000, 10000) (10000, 7509)\n"
     ]
    }
   ],
   "source": [
    "print similarity_matrix_test.shape, bow_data.shape\n",
    "test_bow_data = similarity_matrix_test.dot(bow_data)\n",
    "test_bow_data = preprocessing.scale(test_bow_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 7509) (1000, 7509)\n",
      "(3000, 7509) (1000, 7509)\n"
     ]
    }
   ],
   "source": [
    "# we can use this as our feature vector to do a logistic regression\n",
    "print train_bow_data.shape, test_bow_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 7509) (600, 7509) (2400,) (600,)\n",
      "(2400, 7509) (600, 7509) (2400,) (600,)\n"
     ]
    }
   ],
   "source": [
    "train_indices, test_indices = [], []\n",
    "for train_index, test_index in KFold(len(train_bow_data), n_folds=5):\n",
    "    train_indices = train_index\n",
    "    test_indices = test_index\n",
    "x_train, x_test = train_bow_data[train_indices], train_bow_data[test_indices]\n",
    "y_train, y_test = train_labels[train_indices], train_labels[test_indices]\n",
    "\n",
    "print x_train.shape, x_test.shape, y_train.shape, y_test.shape\n",
    "\n",
    "# save the train and test indices so that the same values are used in the classification section\n",
    "np.savetxt('train_indices.txt', train_indices)\n",
    "np.savetxt('test_indices.txt', test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.291666666667\n",
      "0.291666666667\n"
     ]
    }
   ],
   "source": [
    "# svm classifier on the single fold from training data\n",
    "svmmodel_bow_data = svm.SVC(kernel='linear', probability=True).fit(x_train,y_train) \n",
    "score_bow = svmmodel_bow_data.score(x_test,y_test) \n",
    "print score_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KAGGLE OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# svm classifier\n",
    "svmmodel_bow_data_full = svm.SVC(kernel='linear', probability=True).fit(train_bow_data, train_labels) \n",
    "predictions_cluster_full = svmmodel_bow_data_full.predict_proba(test_bow_data) \n",
    "np.savetxt('predictions_cluster.txt', predictions_cluster_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusionMatrix(ytest,pred):\n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "    np.set_printoptions(precision=2) \n",
    "    print('Confusion matrix')\n",
    "    print(cm)\n",
    "    pl.matshow(cm,cmap=plt.cm.Blues_r)\n",
    "    pl.colorbar()\n",
    "    pl.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sets import Set\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import normalize\n",
    "import random\n",
    "from scipy.spatial import distance\n",
    "import scipy\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import heapq\n",
    "import string\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "from sklearn import svm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
