{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run \"../library.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_train = preprocessing.normalize(np.hstack([train_cnn,train_bow]), axis=0)\n",
    "cluster_test = preprocessing.normalize(np.hstack([test_cnn,test_bow]), axis=0)\n",
    "cluster_tenk = preprocessing.normalize(np.hstack([tenk_cnn,tenk_bow]), axis=0)\n",
    "full_data = np.vstack([cluster_train, cluster_tenk])\n",
    "full_data_test = np.vstack([cluster_test, cluster_tenk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# spectral clustering\n",
    "from sklearn import cluster\n",
    "\n",
    "spectral = cluster.SpectralClustering(n_clusters=200)\n",
    "spectral_output = spectral.fit(full_data)\n",
    "spectral_labels = spectral_output.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spectral clustering test data\n",
    "\n",
    "spectral_test = cluster.SpectralClustering(n_clusters=200)\n",
    "spectral_output_test = spectral_test.fit(full_data_test)\n",
    "spectral_labels_test = spectral_output_test.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is a 13k by 13k affinity matrix. we can use this to get \n",
    "# affinity of training data with tenk data\n",
    "similarity_matrix = spectral.affinity_matrix_[:3000,3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "similarity_matrix_test = spectral_test.affinity_matrix_[:1000,1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "port = PorterStemmer()\n",
    "lmtzr = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "def rstrip(l):\n",
    "    l = ''.join([i if ord(i) < 128 else ' ' for i in l])\n",
    "    return l.rstrip()\n",
    "\n",
    "\n",
    "def preprocess(l):\n",
    "    line = \" \".join(map(rstrip, l)).encode(\"ascii\")\n",
    "    words = tokenizer.tokenize(line)\n",
    "    \n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        # lowercase\n",
    "        new_word = word.lower()\n",
    "        # remove stop words\n",
    "        if new_word not in stopwords:\n",
    "            # stem and lemmatize\n",
    "            new_word = lmtzr.lemmatize(new_word)\n",
    "            new_word = port.stem(new_word)\n",
    "            # add to processed word list\n",
    "            processed_words.append(new_word)\n",
    "            \n",
    "    return processed_words\n",
    "\n",
    "words_data = map(preprocess, captions_json.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_dict_of_words(words_data):\n",
    "    word_set = Set()\n",
    "    for i in range(len(words_data)):\n",
    "        for word in words_data[i]:\n",
    "            word_set.add(word)\n",
    "            \n",
    "    return list(word_set)\n",
    "\n",
    "words_dict = get_dict_of_words(words_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7509"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_word_count_data(words_data, words_dict, ignore = True):    \n",
    "    word_counts_data = []\n",
    "    for i in range(len(words_data)):\n",
    "        word_counts_row = np.zeros(len(words_dict))\n",
    "        \n",
    "        # store list of words for row\n",
    "        word_list = words_data[i]\n",
    "        \n",
    "        # prune to exclude words not in training data's bag of words\n",
    "        if not ignore:\n",
    "            word_list = list(set(word_list).intersection(words_dict))\n",
    "        \n",
    "        # store count of words for above list\n",
    "        word_counts = Counter(word_list)\n",
    "        \n",
    "        for w in word_counts.keys():\n",
    "            index = words_dict.index(w)\n",
    "            word_counts_row[index] = word_counts[w]\n",
    "            \n",
    "        word_counts_data.append(word_counts_row)\n",
    "            \n",
    "    return np.array(word_counts_data)\n",
    "\n",
    "# bag of words data\n",
    "word_counts_data = get_word_count_data(words_data, words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize\n",
    "bow_data = normalize(word_counts_data, axis=1, norm='l1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 10000) (10000, 7509)\n"
     ]
    }
   ],
   "source": [
    "# take dot product\n",
    "print similarity_matrix.shape, bow_data.shape\n",
    "train_bow_data = similarity_matrix.dot(bow_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print similarity_matrix_test.shape, bow_data.shape\n",
    "test_bow_data = similarity_matrix_test.dot(bow_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 7509) (1000, 7509)\n"
     ]
    }
   ],
   "source": [
    "# we can use this as our feature vector to do a logistic regression\n",
    "print train_bow_data.shape, test_bow_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 7509) (600, 7509) (2400,) (600,)\n"
     ]
    }
   ],
   "source": [
    "train_indices, test_indices = [], []\n",
    "for train_index, test_index in KFold(len(train_bow_data), n_folds=5):\n",
    "    train_indices = train_index\n",
    "    test_indices = test_index\n",
    "x_train, x_test = train_bow_data[train_indices], train_bow_data[test_indices]\n",
    "y_train, y_test = train_labels[train_indices], train_labels[test_indices]\n",
    "\n",
    "print x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15833333333333333"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logistic regression classifier\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Train the algorithm using all the training data\n",
    "lr.fit(x_train, y_train)\n",
    "\n",
    "# Accuracy on the training set\n",
    "lr.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KAGGLE OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# logistic regression classifier\n",
    "lr_cluster = LogisticRegression()\n",
    "\n",
    "# Train the algorithm using all the training data\n",
    "lr_cluster.fit(train_bow_data, train_labels)\n",
    "\n",
    "# Accuracy on the training set\n",
    "predictions_cluster = lr_cluster.predict_proba(test_bow_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt('predictions_cluster.txt', predictions_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pctest = np.loadtxt('predictions_cluster.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 200)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pctest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sets import Set\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import normalize\n",
    "import random\n",
    "from scipy.spatial import distance\n",
    "import scipy\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import heapq\n",
    "import string\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import KFold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
